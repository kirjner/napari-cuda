# === YOUR HPC CLUSTER CONFIGURATION ===

# Module options (choose one after testing):
CUDA_MODULE=cuda/12.4.0     # Standalone CUDA
# CUDA_MODULE=nvhpc/24.5    # NVIDIA HPC SDK (may include newer CUDA)

# Primary partition (adjust based on availability and queue time)
GPU_PARTITION=pi_edboyden       # e.g., pi_edboyden, ou_bcs_high, ou_bcs_low

# Resource constraints (tune for your partitions)
SLURM_MEM=32GB
SLURM_CPUS=16
SLURM_TIME=04:00:00

# Storage
HOME_DIR=$HOME
WORK_DIR=$HOME/napari-cuda

# Cluster login (used by ssh tunnel helper)
LOGIN_HOST=login.cluster.mit.edu
CLUSTER_USER=$USER

# === SERVER CONFIGURATION ===
NAPARI_CUDA_HOST=127.0.0.1
NAPARI_CUDA_STATE_PORT=8081
NAPARI_CUDA_PIXEL_PORT=8082
NAPARI_CUDA_METRICS_PORT=8083

# === RUNTIME ENVIRONMENT ===
QT_QPA_PLATFORM=offscreen
PYOPENGL_PLATFORM=egl
CUDA_VISIBLE_DEVICES=0

